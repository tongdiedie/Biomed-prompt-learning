"""
BiomedDPT_Robust (PMC-CLIP backbone)
====================================
BiomedDPT + ä½è´¨é‡ Prompt é²æ£’æ€§å¢å¼ºï¼ˆPMC-CLIP ç‰ˆæœ¬ï¼‰

æ ¸å¿ƒæ”¹è¿›:
åœ¨ L1 æŸå¤±ä¸­æ·»åŠ ä½è´¨é‡ Prompt çº¦æŸï¼Œè®©æ¨¡å‹åŒæ—¶å­¦ä¹ ï¼š
1. ç»†ç²’åº¦è¯­ä¹‰ï¼ˆä»é«˜è´¨é‡ Promptï¼‰
2. æ ¸å¿ƒè¯­ä¹‰ï¼ˆä»ä½è´¨é‡ Promptï¼‰

æŸå¤±å‡½æ•°:
L = L_ce + Î»1 * L_L1_high + Î»2 * L_KL + Î»3 * L_L1_low

æ–‡ä»¶ä½ç½®ï¼štrainers/BiomedDPT_Robust/biomeddpt_robust_pmcclip.py
"""

import copy
import os
import os.path as osp
import numpy as np
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.cuda.amp import GradScaler, autocast

from dassl.engine import TRAINER_REGISTRY, TrainerX
from dassl.utils import load_pretrained_weights, load_checkpoint
from dassl.optim import build_optimizer, build_lr_scheduler
from dassl.metrics import compute_accuracy

# å¯¼å…¥ Prompt æ¨¡æ¿
from trainers.prompt_templates import (
    BIOMEDDPT_TEMPLATES,        # é«˜è´¨é‡ GPT-4 Prompt
    CUSTOM_BIOMEDDPT_TEMPLATES, # ä¸­ç­‰è´¨é‡æ¨¡æ¿
    ZERO_SHOT_TEMPLATES         # ã€æ–°å¢ã€‘ä½è´¨é‡ Prompt
)

from transformers import AutoTokenizer
import requests
from tqdm import tqdm


def load_pmcclip_to_cpu():
    """åŠ è½½ PMC-CLIP æ¨¡å‹åˆ° CPU"""
    print("ğŸ“¦ åŠ è½½ PMC-CLIP (ResNet50) æ¨¡å‹...")
    
    directory = "clip/checkpoints"
    os.makedirs(directory, exist_ok=True)
    
    # PMC-CLIP æ¨¡å‹æ–‡ä»¶ä¸‹è½½é“¾æ¥
    pmcclip_files = {
        "text_encoder.pth": "https://huggingface.co/axiong/pmc_oa_beta/resolve/main/checkpoint.pt",
        "image_encoder(resnet50).pth": "https://huggingface.co/axiong/pmc_oa_beta/resolve/main/model.pth",
        "text_projection_layer.pth": "https://huggingface.co/axiong/pmc_oa_beta/resolve/main/projection.pth"
    }
    
    # æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹æ–‡ä»¶
    for filename, url in pmcclip_files.items():
        filepath = os.path.join(directory, filename)
        
        if not os.path.exists(filepath):
            print(f"ä¸‹è½½ {filename}...")
            response = requests.get(url, stream=True)
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, 'wb') as file, tqdm(
                desc=filename,
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as bar:
                for data in response.iter_content(chunk_size=1024):
                    size = file.write(data)
                    bar.update(size)
            print(f"âœ… {filename} ä¸‹è½½å®Œæˆ")
        else:
            print(f"âœ… {filename} å·²å­˜åœ¨")
    
    # ä¸‹è½½ tokenizer
    tokenizer_path = os.path.join(directory, "BiomedNLP-BiomedBERT-base-uncased-abstract")
    if not os.path.exists(tokenizer_path):
        print("ä¸‹è½½ BiomedBERT tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract",
            cache_dir=tokenizer_path
        )
    else:
        print("âœ… BiomedBERT tokenizer å·²å­˜åœ¨")
    
    # æ„å»ºæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦æ ¹æ® PMC-CLIP æ¶æ„è°ƒæ•´ï¼‰
    import torchvision.models as models
    
    class PMCCLIPModel:
        def __init__(self):
            # å›¾åƒç¼–ç å™¨ï¼ˆResNet50ï¼‰
            self.image_encoder = models.resnet50(pretrained=False)
            self.image_encoder.fc = nn.Identity()  # ç§»é™¤åˆ†ç±»å±‚
            image_state_dict = torch.load(
                os.path.join(directory, "image_encoder(resnet50).pth"),
                map_location="cpu"
            )
            self.image_encoder.load_state_dict(image_state_dict)
            
            # æ–‡æœ¬ç¼–ç å™¨ï¼ˆBiomedBERTï¼‰
            from transformers import AutoModel
            self.text_encoder = AutoModel.from_pretrained(
                "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract",
                cache_dir=tokenizer_path
            )
            text_state_dict = torch.load(
                os.path.join(directory, "text_encoder.pth"),
                map_location="cpu"
            )
            self.text_encoder.load_state_dict(text_state_dict)
            
            # æŠ•å½±å±‚
            self.text_projection = nn.Linear(768, 2048)
            proj_state_dict = torch.load(
                os.path.join(directory, "text_projection_layer.pth"),
                map_location="cpu"
            )
            self.text_projection.load_state_dict(proj_state_dict)
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract",
                cache_dir=tokenizer_path
            )
            
            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
            self.dtype = torch.float32
        
        def encode_text(self, text_inputs):
            """ç¼–ç æ–‡æœ¬"""
            outputs = self.text_encoder(**text_inputs)
            text_features = outputs.last_hidden_state[:, 0, :]  # [CLS] token
            text_features = self.text_projection(text_features)
            return text_features
        
        def encode_image(self, images):
            """ç¼–ç å›¾åƒ"""
            return self.image_encoder(images)
    
    model = PMCCLIPModel()
    return model


class TextEncoder(nn.Module):
    """æ–‡æœ¬ç¼–ç å™¨ï¼ˆPMC-CLIP çš„ BiomedBERTï¼‰"""
    def __init__(self, clip_model):
        super().__init__()
        self.text_encoder = clip_model.text_encoder
        self.text_projection = clip_model.text_projection
        self.tokenizer = clip_model.tokenizer
        self.dtype = clip_model.dtype

    def forward(self, prompts, tokenized_prompts=None):
        """
        å‰å‘ä¼ æ’­
        
        æ³¨æ„ï¼šPMC-CLIP ä½¿ç”¨ BiomedBERT tokenizerï¼Œä¸åŒäº CLIP
        """
        # å¦‚æœ prompts æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼Œå…ˆ tokenize
        if isinstance(prompts, list):
            text_inputs = self.tokenizer(
                prompts,
                padding=True,
                truncation=True,
                max_length=77,
                return_tensors="pt"
            ).to(next(self.text_encoder.parameters()).device)
        else:
            # å¦‚æœæ˜¯é¢„ç¼–ç çš„åµŒå…¥ï¼Œç›´æ¥ä½¿ç”¨
            text_inputs = {"input_ids": prompts}
        
        # æå–æ–‡æœ¬ç‰¹å¾
        outputs = self.text_encoder(**text_inputs)
        text_features = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        text_features = self.text_projection(text_features)
        
        return text_features


class PromptLearner(nn.Module):
    """
    é²æ£’æ€§å¢å¼ºçš„ Prompt å­¦ä¹ å™¨ï¼ˆPMC-CLIP ç‰ˆæœ¬ï¼‰
    
    åŒ…å«:
    1. é«˜è´¨é‡ Promptï¼ˆæ•™å¸ˆï¼Œå†»ç»“ï¼‰ï¼šGPT-4 ç”Ÿæˆ
    2. ä½è´¨é‡ Promptï¼ˆå‚è€ƒé”šç‚¹ï¼Œå†»ç»“ï¼‰ï¼šç±»åˆ«å
    3. å¯å­¦ä¹  Promptï¼ˆå­¦ç”Ÿï¼‰ï¼šéœ€åŒæ—¶å‘é«˜è´¨é‡å’Œä½è´¨é‡å¯¹é½
    
    æ³¨æ„ï¼šPMC-CLIP ä½¿ç”¨ BiomedBERT tokenizerï¼Œå¤„ç†æ–¹å¼ä¸åŒäº CLIP
    """
    def __init__(self, cfg, classnames, clip_model):
        super().__init__()
        self.cfg = cfg
        self.classnames = classnames
        self.n_cls = len(classnames)
        self.n_ctx = cfg.TRAINER.BIOMEDDPT_ROBUST.N_CTX
        self.dtype = clip_model.dtype
        self.tokenizer = clip_model.tokenizer
        
        # ========== 1. åˆå§‹åŒ–å¯å­¦ä¹  Promptï¼ˆå­¦ç”Ÿï¼‰==========
        ctx_init = cfg.TRAINER.BIOMEDDPT_ROBUST.CTX_INIT
        
        if ctx_init and self.n_ctx <= 4:
            ctx_init = ctx_init.replace("_", " ")
            prompt_prefix = ctx_init
        else:
            prompt_prefix = " ".join(["X"] * self.n_ctx)
        
        print(f'[INIT] Learnable Prompt: \"{prompt_prefix}\"')
        print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {self.n_ctx}")
        
        # ä½¿ç”¨ä¸­ç­‰è´¨é‡æ¨¡æ¿æ„é€ å¯å­¦ä¹  Prompt
        classnames = [name.replace("_", " ") for name in classnames]
        temp = CUSTOM_BIOMEDDPT_TEMPLATES[cfg.DATASET.NAME]
        self.prompts_template = [temp.format(c.replace("_", " ")) for c in classnames]
        
        # å¯¹äº PMC-CLIPï¼Œæˆ‘ä»¬ç›´æ¥ä¼˜åŒ–æ–‡æœ¬è¡¨ç¤º
        # è¿™é‡Œç®€åŒ–ä¸ºå¯å­¦ä¹ çš„åµŒå…¥å‘é‡
        self.ctx = nn.Parameter(torch.randn(self.n_cls, 768, dtype=self.dtype))  # 768 æ˜¯ BiomedBERT çš„éšè—ç»´åº¦
        nn.init.normal_(self.ctx, std=0.02)
        
        # ========== 2. åŠ è½½é«˜è´¨é‡ Promptï¼ˆæ•™å¸ˆï¼Œå†»ç»“ï¼‰==========
        print("[TEACHER] Loading high-quality Prompt (GPT-4 generated, frozen)")
        
        with torch.no_grad():
            # é¢„è®¡ç®—é«˜è´¨é‡ Prompt çš„ç‰¹å¾
            all_teacher_features = []
            for i in range(cfg.TRAINER.BIOMEDDPT_ROBUST.N_PROMPTS):
                high_quality_prompts = [
                    BIOMEDDPT_TEMPLATES[classname][i] 
                    for classname in classnames
                ]
                text_inputs = self.tokenizer(
                    high_quality_prompts,
                    padding=True,
                    truncation=True,
                    max_length=77,
                    return_tensors="pt"
                ).to("cuda")
                
                text_features = clip_model.encode_text(text_inputs)
                all_teacher_features.append(text_features.cpu().unsqueeze(1))

        self.fixed_embeddings = torch.cat(all_teacher_features, dim=1)  # é«˜è´¨é‡ç‰¹å¾
        print(f"[OK] High-quality Prompts: {cfg.TRAINER.BIOMEDDPT_ROBUST.N_PROMPTS} per class")
        
        # ========== 3. ã€å…³é”®æ–°å¢ã€‘åˆå§‹åŒ–ä½è´¨é‡ Promptï¼ˆé²æ£’æ€§é”šç‚¹ï¼Œå†»ç»“ï¼‰==========
        print("[ANCHOR] Loading low-quality Prompt (robustness anchor, frozen)")
        low_template_type = cfg.TRAINER.BIOMEDDPT_ROBUST.LOW_TEMPLATE_TYPE
        
        if low_template_type not in ZERO_SHOT_TEMPLATES:
            print(f"è­¦å‘Š: æœªçŸ¥æ¨¡æ¿ç±»å‹ '{low_template_type}'ï¼Œä½¿ç”¨ 'minimal'")
            low_template_type = "minimal"
        
        template = ZERO_SHOT_TEMPLATES[low_template_type]
        print(f"ä½è´¨é‡æ¨¡æ¿ç±»å‹: {low_template_type}")
        
        # ç”Ÿæˆä½è´¨é‡ Prompt
        if template == "":
            low_quality_prompts = ["" for _ in classnames]
            print("ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºä½è´¨é‡ Prompt")
        else:
            low_quality_prompts = [template.format(**{"class": cls}) for cls in classnames]
            print(f"ç”Ÿæˆçš„ä½è´¨é‡ Prompt ç¤ºä¾‹:")
            for cls, prompt in zip(classnames[:3], low_quality_prompts[:3]):
                print(f"  {cls:15} -> '{prompt}'")
        
        # é¢„è®¡ç®—ä½è´¨é‡ Prompt çš„ç‰¹å¾
        with torch.no_grad():
            text_inputs = self.tokenizer(
                [p if p else "X" for p in low_quality_prompts],
                padding=True,
                truncation=True,
                max_length=77,
                return_tensors="pt"
            ).to("cuda")
            
            low_text_features = clip_model.encode_text(text_inputs)
        
        self.fixed_low_embeddings = low_text_features.cpu()  # ä½è´¨é‡ç‰¹å¾ï¼ˆå†»ç»“ï¼‰
        print(f"[OK] Low-quality Prompt initialized")

    def forward(self):
        """
        è¿”å›å¯å­¦ä¹  Prompt çš„æ–‡æœ¬åˆ—è¡¨
        
        è¿”å›:
            prompts: å¯å­¦ä¹  Prompt æ–‡æœ¬åˆ—è¡¨
        """
        # è¿”å›æ¨¡æ¿æ–‡æœ¬ï¼ˆå®é™…è®­ç»ƒæ—¶ä¼šé€šè¿‡ ctx è°ƒæ•´è¡¨ç¤ºï¼‰
        return self.prompts_template


class CLIP_Inplanted(nn.Module):
    """å¸¦ Visual Prompt çš„å›¾åƒç¼–ç å™¨ï¼ˆPMC-CLIP ç‰ˆæœ¬ï¼ŒResNet50ï¼‰"""
    def __init__(self, clip_model):
        super().__init__()
        self.image_encoder = clip_model.image_encoder
        self.dtype = clip_model.dtype
        
        # Visual Prompt å‚æ•°ï¼ˆè°ƒæ•´ä¸º ResNet50 çš„è¾“å…¥ç»´åº¦ï¼‰
        self.num_tokens = 4
        self.prompt_dim = 2048  # ResNet50 çš„è¾“å‡ºç»´åº¦
        
        # æ³¨æ„ï¼šå¯¹äº ResNetï¼ŒVisual Prompt çš„æ³¨å…¥æ–¹å¼éœ€è¦è°ƒæ•´
        # è¿™é‡Œç®€åŒ–ä¸ºåœ¨ç‰¹å¾å±‚é¢æ·»åŠ å¯å­¦ä¹ çš„åç½®
        self.prompt_bias = nn.Parameter(torch.zeros(1, self.prompt_dim))
        nn.init.normal_(self.prompt_bias, std=0.02)

    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼ˆResNet50ï¼‰"""
        features = self.image_encoder(x)
        
        # æ·»åŠ å¯å­¦ä¹ çš„ Visual Promptï¼ˆç®€åŒ–ç‰ˆï¼‰
        features = features + self.prompt_bias
        
        return features


class CustomCLIP(nn.Module):
    """é²æ£’æ€§å¢å¼ºçš„ PMC-CLIP æ¨¡å‹"""
    def __init__(self, cfg, classnames, clip_model):
        super().__init__()
        self.prompt_learner = PromptLearner(cfg, classnames, clip_model)
        self.image_encoder = CLIP_Inplanted(clip_model)
        self.text_encoder = TextEncoder(clip_model)
        self.logit_scale = clip_model.logit_scale
        self.dtype = clip_model.dtype
        self.total_epochs = cfg.OPTIM.MAX_EPOCH
        self.n_cls = len(classnames)
        self.cfg = cfg

    def forward(self, image, label=None):
        """
        å‰å‘ä¼ æ’­
        
        è®¡ç®—æŸå¤±:
        L = L_ce + Î»1 * L_L1_high + Î»2 * L_KL + Î»3 * L_L1_low
        """
        logit_scale = self.logit_scale.exp()

        # è·å–å¯å­¦ä¹  Promptï¼ˆæ–‡æœ¬åˆ—è¡¨ï¼‰
        prompts = self.prompt_learner()

        # æå–ç‰¹å¾
        text_features = self.text_encoder(prompts)
        
        # æ·»åŠ å¯å­¦ä¹ çš„ä¸Šä¸‹æ–‡è°ƒæ•´
        text_features = text_features + self.prompt_learner.ctx
        
        image_features = self.image_encoder(image.type(self.dtype))
        
        # å½’ä¸€åŒ–
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        # é«˜è´¨é‡ç‰¹å¾ï¼ˆæ•™å¸ˆï¼‰
        fixed_embeddings = self.prompt_learner.fixed_embeddings
        fixed_embeddings = fixed_embeddings / fixed_embeddings.norm(dim=-1, keepdim=True)
        fixed_embeddings = fixed_embeddings.mean(dim=1)
        fixed_embeddings = fixed_embeddings / fixed_embeddings.norm(dim=-1, keepdim=True)
        
        # ã€å…³é”®æ–°å¢ã€‘ä½è´¨é‡ç‰¹å¾ï¼ˆé²æ£’æ€§é”šç‚¹ï¼‰
        fixed_low_embeddings = self.prompt_learner.fixed_low_embeddings
        fixed_low_embeddings = fixed_low_embeddings / fixed_low_embeddings.norm(dim=-1, keepdim=True)
        
        # è®¡ç®— logits
        zero_shot_logits = logit_scale * image_features @ fixed_embeddings.cuda().t()
        logits = logit_scale * image_features @ text_features.t()
        
        if self.prompt_learner.training:
            # ========== æŸå¤± 1ï¼šäº¤å‰ç†µæŸå¤± ==========
            loss_ce = F.cross_entropy(logits, label)
            
            # ========== æŸå¤± 2ï¼šL1 å¯¹é½æŸå¤±ï¼ˆå¯å­¦ä¹  â†’ é«˜è´¨é‡ï¼‰==========
            loss_l1_high = F.l1_loss(
                text_features, 
                fixed_embeddings.cuda(), 
                reduction='mean'
            ) * self.cfg.TRAINER.BIOMEDDPT_ROBUST.L1_LAMBDA_HIGH
            
            # ========== æŸå¤± 3ï¼šKL æ•£åº¦æŸå¤±ï¼ˆçŸ¥è¯†è’¸é¦ï¼‰==========
            loss_kl = F.kl_div(
                F.log_softmax(logits, dim=1),
                F.log_softmax(zero_shot_logits, dim=1),
                reduction='sum',
                log_target=True
            ) / logits.numel() * self.cfg.TRAINER.BIOMEDDPT_ROBUST.KL_LAMBDA
            
            # ========== ã€å…³é”®æ–°å¢ã€‘æŸå¤± 4ï¼šL1 é²æ£’æ€§çº¦æŸï¼ˆå¯å­¦ä¹  â†’ ä½è´¨é‡ï¼‰==========
            loss_l1_low = F.l1_loss(
                text_features, 
                fixed_low_embeddings.cuda(), 
                reduction='mean'
            ) * self.cfg.TRAINER.BIOMEDDPT_ROBUST.L1_LAMBDA_LOW
            
            # ========== æ€»æŸå¤± ==========
            total_loss = loss_ce + loss_l1_high + loss_kl + loss_l1_low
            
            return logits, total_loss
        else:
            return logits


@TRAINER_REGISTRY.register()
class BiomedDPT_Robust_PMCCLIP(TrainerX):
    """BiomedDPT_Robust è®­ç»ƒå™¨ï¼ˆPMC-CLIP backboneï¼‰"""
    
    def check_cfg(self, cfg):
        assert cfg.TRAINER.BIOMEDDPT_ROBUST.PREC in ["fp16", "fp32", "amp"]

    def build_model(self):
        cfg = self.cfg
        classnames = self.dm.dataset.classnames

        print(f"\n{'='*80}")
        print(f"ğŸš€ æ„å»º BiomedDPT_Robust æ¨¡å‹ï¼ˆPMC-CLIP backboneï¼‰")
        print(f"{'='*80}\n")
        
        print(f"åŠ è½½ PMC-CLIP (ResNet50 + BiomedBERT)")
        clip_model = load_pmcclip_to_cpu()

        print("æ„å»ºè‡ªå®šä¹‰ PMC-CLIP æ¨¡å‹")
        self.model = CustomCLIP(cfg, classnames, clip_model)

        print("å†»ç»“å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œä»…ä¼˜åŒ– Prompt")
        names_to_update = ["prompt_learner.ctx"]

        for name, param in self.model.named_parameters():
            if name not in names_to_update:
                param.requires_grad_(False)

        # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
        enabled = set()
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                enabled.add(name)
        print(f"\n[OK] Trainable parameters: {enabled}")
        print(f"âœ… å‚æ•°æ•°é‡: {len(enabled)}\n")
        
        if cfg.MODEL.INIT_WEIGHTS:
            load_pretrained_weights(self.model, cfg.MODEL.INIT_WEIGHTS)

        self.model.to(self.device)
        self.optim = build_optimizer(self.model, cfg.OPTIM)
        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)
        self.register_model("prompt_learner", self.model, self.optim, self.sched)
        
        self.total_epochs = cfg.OPTIM.MAX_EPOCH
        self.step_counter = 1
        self.scaler = GradScaler() if cfg.TRAINER.BIOMEDDPT_ROBUST.PREC == "amp" else None
        
        device_count = torch.cuda.device_count()
        if device_count > 1:
            print(f"æ£€æµ‹åˆ°å¤š GPU ({device_count} ä¸ª)ï¼Œä½¿ç”¨å…¨éƒ¨ï¼")
            self.model = nn.DataParallel(self.model)
        
        print(f"{'='*80}\n")

    def forward_backward(self, batch):
        image, label = self.parse_batch_train(batch)

        model = self.model
        optim = self.optim
        scaler = self.scaler

        prec = self.cfg.TRAINER.BIOMEDDPT_ROBUST.PREC
        if prec == "amp":
            with autocast():
                loss = model(image, label)
            optim.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optim)
            scaler.update()
        else:
            logits, loss = model(image, label)
            self.model_backward_and_update(loss)

        loss_summary = {
            "loss": loss.item(),
            "acc": compute_accuracy(logits, label)[0].item(),
        }

        if (self.batch_idx + 1) == self.num_batches:
            self.update_lr()

        return loss_summary

    def parse_batch_train(self, batch):
        input = batch["img"]
        label = batch["label"]
        input = input.to(self.device)
        label = label.to(self.device)
        return input, label

    def load_model(self, directory, epoch=None):
        if not directory:
            print("Note that load_model() is skipped as no pretrained model is given")
            return

        names = self.get_model_names()
        model_file = "model-best.pth.tar"

        if epoch is not None:
            model_file = "model.pth.tar-" + str(epoch)

        for name in names:
            model_path = osp.join(directory, name, model_file)

            if not osp.exists(model_path):
                raise FileNotFoundError('Model not found at "{}"'.format(model_path))

            checkpoint = load_checkpoint(model_path)
            state_dict = checkpoint["state_dict"]
            epoch = checkpoint["epoch"]

            print("Loading weights to {} " 'from "{}" (epoch = {})'.format(name, model_path, epoch))
            self._models[name].load_state_dict(state_dict, strict=False)
